import os
from dotenv import load_dotenv
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, length, col, to_json
import pandas as pd
import json
import ast
from typing import List, Dict
import re
import logging
from collections import defaultdict

# Charger les variables d'environnement
load_dotenv()

# V√©rifier si les identifiants Azure sont bien d√©finis
storage_account_name = os.getenv("AZURE_STORAGE_ACCOUNT_NAME")
storage_account_key = os.getenv("AZURE_STORAGE_ACCOUNT_KEY")
container_name = os.getenv("AZURE_CONTAINER_NAME")
parquet_folder = "data/"

if not storage_account_name or not storage_account_key or not container_name:
    raise ValueError("‚ùå ERREUR : V√©rifie que les variables AZURE_STORAGE_ACCOUNT_NAME, AZURE_STORAGE_ACCOUNT_KEY et AZURE_CONTAINER_NAME sont bien d√©finies dans .env.")

# Chemin vers les fichiers JAR Hadoop et Azure
hadoop_jars_path = os.path.expanduser("~/hadoop_jars")

# Liste des JARs n√©cessaires
jars = [
    f"{hadoop_jars_path}/hadoop-azure-3.3.1.jar",
    f"{hadoop_jars_path}/azure-storage-8.6.6.jar",
    f"{hadoop_jars_path}/jetty-util-9.4.40.v20210413.jar",
    f"{hadoop_jars_path}/jetty-util-ajax-9.4.40.v20210413.jar"
]

# Configuration Spark optimis√©e avec plus de m√©moire
spark = SparkSession.builder \
    .appName("AzureParquetAnalysis") \
    .master("local[*]") \
    .config("spark.jars", ",".join(jars)) \
    .config("spark.hadoop.fs.azure", "org.apache.hadoop.fs.azure.NativeAzureFileSystem") \
    .config(f"spark.hadoop.fs.azure.account.key.{storage_account_name}.blob.core.windows.net", storage_account_key) \
    .config("spark.hadoop.fs.azure.account.auth.type", "SharedKey") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.maxResultSize", "2g") \
    .config("spark.driver.host", "localhost") \
    .getOrCreate()

print("‚úÖ PySpark est bien configur√© avec les fichiers JAR pour Azure !")

# Ajouter les patterns regex au d√©but du fichier, apr√®s les imports
TRANSLATION_PATTERNS = {
    "fr_dr": re.compile(r'\"content\":\"ÿ™ÿ±ÿ¨ŸÖ ŸÖŸÜ ÿßŸÑŸÅÿ±ŸÜÿ≥ÿßŸàŸäÿ© ŸÑŸÑÿØÿßÿ±ÿ¨ÿ©:\\\\n(.*?)\".*?\"content\":\"(.*?)\"'),
    "dr_fr": re.compile(r'\"content\":\"(?:ÿ™ÿ±ÿ¨ŸÖ ŸÖŸÜ ÿßŸÑÿØÿßÿ±ÿ¨ÿ© ŸÑŸÑŸÅÿ±ŸÜÿ≥ÿßŸàŸäÿ©:|ÿ™ÿ±ÿ¨ŸÖ:)\\\\n?(.*?)\".*?\"content\":\"(.*?)\"'),
    "en_dr": re.compile(r'\"content\":\"ÿ™ÿ±ÿ¨ŸÖ ŸÖŸÜ ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© ŸÑŸÑÿØÿßÿ±ÿ¨ÿ©:\\\\n(.*?)\".*?\"content\":\"(.*?)\"'),
    "dr_en": re.compile(r'\"content\":\"(?:ÿ™ÿ±ÿ¨ŸÖ ŸÖŸÜ ÿßŸÑÿØÿßÿ±ÿ¨ÿ© ŸÑŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ©:|ÿ™ÿ±ÿ¨ŸÖ:)\\\\n?(.*?)\".*?\"content\":\"(.*?)\"')
}

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('translation_parsing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# Statistiques globales pour le logging
parsing_stats = {
    "total_messages": 0,
    "matched_messages": 0,
    "unmatched_messages": 0,
    "matches_by_direction": defaultdict(int),
    "unmatched_details": []  # Nouvelle structure pour les d√©tails
}

def load_and_filter_data():
    """Charge et filtre les donn√©es depuis Azure Blob Storage."""
    # URL du stockage Azure
    azure_url = f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{parquet_folder}"
    print(f"üìÇ Lecture des fichiers Parquet depuis : {azure_url}")
    
    # Lecture des donn√©es
    df = spark.read.parquet(azure_url)
    print("‚úÖ Lecture r√©ussie !")

    # 1. R√©duire le nombre de partitions
    df_optimise = df.coalesce(4)

    # 2. D√©finir les directions √† conserver
    directions_a_garder = ['dr_fr', 'fr_dr', 'en_dr', 'dr_en']

    # 3. Optimisation du filtrage avec cache
    df_filtre = df_optimise \
        .select('dataset', 'id', 'messages', 'direction') \
        .filter(col('direction').isin(directions_a_garder)) \
        .repartition(4, 'direction') \
        .persist()

    # 4. Cr√©er le DataFrame plat avec messages_json
    df_plat = df_filtre \
        .withColumn("messages_json", to_json(col("messages"))) \
        .drop("messages") \
        .select("dataset", "id", "messages_json", "direction") \
        .toPandas()

    print(f"üìä Nombre de lignes : {len(df_plat):,}")
    print("\nüìä Distribution des directions :")
    print(df_plat['direction'].value_counts())

    return df_plat

# Dictionnaire direction ‚Üí langues
DIRECTION_MAPPING = {
    "en_dr": ("en", "darija"),
    "fr_dr": ("fr", "darija"),
    "dr_fr": ("darija", "fr"),
    "dr_en": ("darija", "en")
}

def parse_messages(messages_str: str) -> List[Dict[str, str]]:
    """
    Parse les messages JSON en une liste de dictionnaires en utilisant des regex
    pour extraire directement les paires de traduction.
    """
    try:
        processed_messages = []
        message_matched = False
        parsing_stats["total_messages"] += 1
        
        # Pour chaque direction possible
        for direction, pattern in TRANSLATION_PATTERNS.items():
            # Chercher toutes les correspondances dans le texte
            matches = pattern.finditer(messages_str)
            matches_found = False
            
            for match in matches:
                matches_found = True
                message_matched = True
                source_text = match.group(1).strip()
                target_text = match.group(2).strip()
                
                # Nettoyer les caract√®res d'√©chappement JSON
                source_text = source_text.replace('\\n', '\n').replace('\\\"', '"').replace('\\\\', '\\')
                target_text = target_text.replace('\\n', '\n').replace('\\\"', '"').replace('\\\\', '\\')
                
                # Enlever les balises <|assistant|> si pr√©sentes
                target_text = re.sub(r'<\|assistant\|>', '', target_text).strip()
                
                if source_text and target_text:
                    parsing_stats["matches_by_direction"][direction] += 1
                    processed_messages.extend([
                        {"role": "user", "content": source_text, "direction": direction},
                        {"role": "assistant", "content": target_text}
                    ])
                    logging.debug(f"Match trouv√© pour {direction}:")
                    logging.debug(f"Source: {source_text[:100]}...")
                    logging.debug(f"Target: {target_text[:100]}...")
        
        if not message_matched:
            parsing_stats["unmatched_messages"] += 1
            # Stocker les d√©tails complets du message non match√©
            try:
                # Essayer de parser le JSON pour une meilleure analyse
                message_data = json.loads(messages_str)
                unmatched_detail = {
                    "raw_message": messages_str[:1000],  # Premiers 1000 caract√®res
                    "parsed_structure": message_data,
                    "potential_issues": []
                }
                
                # Analyser les probl√®mes potentiels
                if "content" not in str(message_data):
                    unmatched_detail["potential_issues"].append("Pas de champ 'content' trouv√©")
                if "ÿ™ÿ±ÿ¨ŸÖ" not in str(message_data):
                    unmatched_detail["potential_issues"].append("Pas d'instruction de traduction trouv√©e")
                if "\\n" not in str(message_data):
                    unmatched_detail["potential_issues"].append("Pas de retour √† la ligne trouv√©")
                
                parsing_stats["unmatched_details"].append(unmatched_detail)
                
            except json.JSONDecodeError:
                # Si le parsing JSON √©choue, stocker le message brut
                parsing_stats["unmatched_details"].append({
                    "raw_message": messages_str[:1000],
                    "error": "Invalid JSON format",
                    "potential_issues": ["Format JSON invalide"]
                })
            
            logging.warning(f"Aucun match trouv√© pour le message: {messages_str[:100]}...")
        else:
            parsing_stats["matched_messages"] += 1
        
        return processed_messages

    except Exception as e:
        logging.error(f"‚ùå Erreur lors du parsing des messages : {str(e)}")
        parsing_stats["unmatched_details"].append({
            "raw_message": messages_str[:1000],
            "error": str(e),
            "potential_issues": ["Erreur lors du traitement"]
        })
        return []

def print_parsing_stats():
    """
    Affiche et enregistre les statistiques de parsing.
    """
    logging.info("\n=== Statistiques de Parsing ===")
    logging.info(f"Messages totaux trait√©s: {parsing_stats['total_messages']}")
    logging.info(f"Messages match√©s: {parsing_stats['matched_messages']}")
    logging.info(f"Messages non match√©s: {parsing_stats['unmatched_messages']}")
    logging.info("\nMatches par direction:")
    for direction, count in parsing_stats["matches_by_direction"].items():
        logging.info(f"- {direction}: {count}")
    
    logging.info("\nExemples de messages non match√©s:")
    for i, sample in enumerate(parsing_stats["unmatched_details"], 1):
        logging.info(f"\n{i}. {sample['raw_message'][:100]}...")

def clean_translations(df: pd.DataFrame):
    """
    Nettoie et structure les traductions √† partir du DataFrame.
    """
    valid_data = []
    invalid_data = []
    cleaned_count = 0
    quality_issues = []
    messages_with_multiple_pairs = 0
    multi_turns_count = 0

    # Liste des r√©ponses courtes acceptables
    SHORT_ANSWERS = {
        "fr": ["oui", "non", "ah", "oh", "nan", "si"],
        "darija": ["ÿ£Ÿá", "ŸÑÿß", "ÿ£ŸèŸá", "ÿ£Ÿè", "ŸÜÿπŸÖ", "ŸÜÿπ"]
    }

    for index, row in df.iterrows():
        try:
            # Utilisation de la colonne "messages_json" √† la place de "messages"
            messages = parse_messages(row["messages_json"])
            direction = row.get("direction")

            # V√©rifier la validit√© des messages
            if not messages or len(messages) < 2:
                invalid_data.append({
                    "index": index,
                    "reason": "Invalid message format",
                    "messages": row["messages_json"]
                })
                continue

            # V√©rifier la validit√© de la direction
            if direction not in DIRECTION_MAPPING:
                invalid_data.append({
                    "index": index,
                    "reason": "Unknown direction value",
                    "direction": direction,
                    "messages": messages
                })
                continue

            source_lang, target_lang = DIRECTION_MAPPING[direction]
            
            # Traiter les paires de messages
            conversation_pairs = []
            for i in range(0, len(messages), 2):
                if i + 1 >= len(messages):
                    break
                    
                user_msg = messages[i]
                assistant_msg = messages[i + 1]
                
                if user_msg.get("role") != "user" or assistant_msg.get("role") != "assistant":
                    continue

                original_text = user_msg["content"].strip()
                translation_text = assistant_msg["content"].strip()

                # V√©rifications de qualit√©
                quality_checks = []
                
                # 1. V√©rifier la longueur minimale
                if len(original_text) < 2 or len(translation_text) < 2:
                    quality_checks.append("Texte trop court")
                
                # 2. V√©rifier si la traduction est vide
                if not translation_text or translation_text.isspace():
                    quality_checks.append("Traduction vide")
                
                # 3. V√©rifier si la traduction est identique √† l'original
                if original_text == translation_text:
                    quality_checks.append("Traduction identique √† l'original")
                
                # 4. V√©rifier la longueur relative
                if len(translation_text) < len(original_text) * 0.3:
                    quality_checks.append("Traduction trop courte")
                
                # 5. V√©rifier les caract√®res selon la direction
                if direction == "fr_dr" or direction == "en_dr":
                    arabic_chars = set("ÿßÿ®ÿ™ÿ´ÿ¨ÿ≠ÿÆÿØÿ∞ÿ±ÿ≤ÿ≥ÿ¥ÿµÿ∂ÿ∑ÿ∏ÿπÿ∫ŸÅŸÇŸÉŸÑŸÖŸÜŸáŸàŸäÿ°ÿ§ÿ¶ÿ£ÿ•ÿ¢ÿ©")
                    non_arabic = [c for c in translation_text if c not in arabic_chars and not c.isspace() and not c.isdigit() and c not in ".,ÿå!ÿü:ÿõ()[]{}"]
                    if non_arabic:
                        quality_checks.append(f"Caract√®res non-arabes : {''.join(non_arabic)}")
                
                # Ajouter les informations de tour
                total_turns = len(messages) // 2
                current_turn = (i // 2) + 1
                quality_checks.append(f"Tour {current_turn}/{total_turns}")

                # Ajouter la paire √† la conversation
                conversation_pairs.append({
                    "source_text": original_text,
                    "target_text": translation_text,
                    "source_lang": source_lang,
                    "target_lang": target_lang,
                    "direction": direction,
                    "quality_checks": quality_checks,
                    "turn": current_turn,
                    "total_turns": total_turns
                })
                cleaned_count += 1

            # Ajouter toutes les paires de la conversation
            valid_data.extend(conversation_pairs)

            # Mettre √† jour les statistiques
            if len(messages) > 2:
                messages_with_multiple_pairs += 1
                multi_turns_count += (len(messages) // 2) - 1

        except Exception as e:
            print(f"‚ùå Erreur lors du traitement de l'index {index}: {str(e)}")
            invalid_data.append({
                "index": index,
                "reason": f"Processing error: {str(e)}",
                "messages": row["messages_json"]
            })

    print(f"\nüìä Statistiques des conversations multi-tours :")
    print(f"- Total des traductions : {cleaned_count}")
    print(f"- Messages avec plusieurs paires : {messages_with_multiple_pairs}")
    print(f"- Nombre total de tours suppl√©mentaires : {multi_turns_count}")
    if messages_with_multiple_pairs > 0:
        print(f"- Nombre moyen de paires par message multi-tour : {cleaned_count/messages_with_multiple_pairs:.2f}")

    return valid_data, invalid_data, cleaned_count, quality_issues

def save_json(data, filename: str):
    """Sauvegarde les donn√©es dans un fichier JSON avec encodage UTF-8."""
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def structure_translations(cleaned_file: str, output_file: str):
    """
    Transforme le JSON nettoy√© en un format structur√© explicite 
    avec les champs 'source_lang', 'source', 'target_lang' et 'target'.
    """
    with open(cleaned_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    structured_translations = []
    for item in data.get("translations", []):
        structured_translations.append({
            "source_lang": item.get("source_lang"),
            "source": item.get("source_text"),
            "target_lang": item.get("target_lang"),
            "target": item.get("target_text"),
            "direction": item.get("direction"),
            "quality_checks": item.get("quality_checks", []),
            "turn": item.get("turn", 1),
            "total_turns": item.get("total_turns", 1)
        })

    # Afficher les statistiques sur les conversations multi-tours
    multi_turn_conversations = sum(1 for t in structured_translations if t["total_turns"] > 1)
    total_turns = sum(t["turn"] for t in structured_translations)
    max_turns = max(t["total_turns"] for t in structured_translations) if structured_translations else 0

    print(f"\nüìä Statistiques des conversations structur√©es :")
    print(f"- Total des traductions : {len(structured_translations)}")
    print(f"- Conversations multi-tours : {multi_turn_conversations}")
    print(f"- Nombre total de tours : {total_turns}")
    print(f"- Nombre maximum de tours dans une conversation : {max_turns}")

    save_json({"translations": structured_translations}, output_file)

def save_to_csv(df: pd.DataFrame, output_dir: str):
    """Sauvegarde le DataFrame en format CSV."""
    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, "translations.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"‚úÖ DataFrame sauvegard√© en CSV dans : {csv_path}")

def save_unmatched_details(output_file: str):
    """
    Sauvegarde les d√©tails des messages non match√©s dans un fichier JSON.
    """
    unmatched_data = {
        "statistics": {
            "total_messages": parsing_stats["total_messages"],
            "matched_messages": parsing_stats["matched_messages"],
            "unmatched_messages": parsing_stats["unmatched_messages"],
            "matches_by_direction": dict(parsing_stats["matches_by_direction"])
        },
        "unmatched_details": parsing_stats["unmatched_details"]
    }
    
    save_json(unmatched_data, output_file)
    print(f"\nüíæ D√©tails des messages non match√©s sauvegard√©s dans : {output_file}")

if __name__ == "__main__":
    print("üöÄ D√©marrage du traitement des donn√©es...")
    
    # Charger et filtrer les donn√©es depuis Azure
    print("\nüì• Chargement des donn√©es depuis Azure...")
    merged_df = load_and_filter_data()
    
    # Sauvegarder en CSV
    csv_output_dir = "/projets/darija_app/data_Darija-SFT-Mixture/darija_data/csv_files"
    print("\nüíæ Sauvegarde du DataFrame en CSV...")
    save_to_csv(merged_df, csv_output_dir)
    
    print("\nüßπ Nettoyage des donn√©es...")
    valid_data, invalid_data, cleaned_count, quality_issues = clean_translations(merged_df)

    # Afficher les statistiques de parsing
    print_parsing_stats()

    # Obtenir le chemin du dossier courant (agregation)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Cr√©er le dossier structured_json s'il n'existe pas
    structured_json_dir = os.path.join(current_dir, "structured_json")
    os.makedirs(structured_json_dir, exist_ok=True)

    # D√©finir les chemins des fichiers de sortie dans le dossier structured_json
    cleaned_file = os.path.join(structured_json_dir, "cleaned_translations.json")
    invalid_file = os.path.join(structured_json_dir, "invalid_translations.json")
    structured_file = os.path.join(structured_json_dir, "structured_translations.json")
    quality_file = os.path.join(structured_json_dir, "quality_issues.json")
    unmatched_file = os.path.join(structured_json_dir, "unmatched_messages.json")

    print("\nüíæ Sauvegarde des r√©sultats...")
    save_json({"translations": valid_data}, cleaned_file)
    save_json(invalid_data, invalid_file)
    save_json(quality_issues, quality_file)

    print(f"\n‚úÖ Nettoyage termin√© ! {cleaned_count} entr√©es nettoy√©es et conserv√©es.")
    print(f"‚ö†Ô∏è {len(invalid_data)} entr√©es bruit√©es d√©tect√©es et enregistr√©es dans {invalid_file}")
    print(f"‚ö†Ô∏è {len(quality_issues)} probl√®mes de qualit√© d√©tect√©s et enregistr√©s dans {quality_file}")

    if invalid_data:
        print("\nüîé Exemples d'erreurs d√©tect√©es :")
        for error in invalid_data[:5]:
            print(f"- Index {error['index']} : {error['reason']}")
            print(f"  Messages : {error['messages']}\n")

    # Structuration finale des traductions nettoy√©es
    print("\nüîÑ Structuration finale des donn√©es...")
    structure_translations(cleaned_file, structured_file)

    # Sauvegarder les d√©tails des messages non match√©s
    save_unmatched_details(unmatched_file)

    print("\n‚ú® Traitement termin√© avec succ√®s !")

    # Arr√™ter la session Spark
    spark.stop()
